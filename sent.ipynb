{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR='raw'\n",
    "data_names = os.listdir(DATA_DIR)\n",
    "data_names = [name[:-4] for name in data_names if name != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def load_markdown_file(file_path):\n",
    "    with open(file_path, \"r\", encoding='UTF-8') as stream:\n",
    "        markdown_str = stream.read()\n",
    "        return markdown_str\n",
    "\n",
    "def _add_sentence_to_list(sentence: str, sentences_list):\n",
    "    \"\"\"\n",
    "    Add a sentence to the list of sentences.\n",
    "    Args:\n",
    "        sentence (str):\n",
    "            Sentence to be added.\n",
    "        sentences (List[str]):\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    while sentence.startswith(\" \"):\n",
    "        # remove leading space\n",
    "        sentence = sentence[1:]\n",
    "    if all(c in punctuation for c in sentence) or len(sentence) == 1:\n",
    "        # skip sentences with only punctuation\n",
    "        return\n",
    "    sentences_list.append(sentence)\n",
    "\n",
    "def get_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "    Args:\n",
    "        text (str):\n",
    "            Text to be processed.\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    # get the paragraphs\n",
    "    text=   re.sub(\" \\d+\\n\", \".\", text)\n",
    "    text=   re.sub(\"\\n\\d+\", \" \", text)\n",
    "    text=   re.sub(\"\\n\", \" \", text)\n",
    "    text=   re.sub(\"\\d+.\", \"\", text)\n",
    "    paragraphs = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    paragraphs = [p for p in paragraphs if p != \"\"]\n",
    "    # get the sentences from the paragraphs\n",
    "    sentences = list()\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.startswith(\"#\"):\n",
    "            _add_sentence_to_list(paragraph, sentences)\n",
    "            continue\n",
    "        prev_sentence_idx = 0\n",
    "        for idx in range(len(paragraph)):\n",
    "            if idx + 1 < len(paragraph):\n",
    "                if (paragraph[idx] == \".\" and not paragraph[idx + 1].isdigit()) or (\n",
    "                    paragraph[idx] in \"!?\"\n",
    "                ):\n",
    "                    sentence = paragraph[prev_sentence_idx : idx + 1]\n",
    "                    _add_sentence_to_list(sentence, sentences)\n",
    "                    prev_sentence_idx = idx + 1\n",
    "            else:\n",
    "                sentence = paragraph[prev_sentence_idx:]\n",
    "                _add_sentence_to_list(sentence, sentences)\n",
    "    return sentences\n",
    "\n",
    "def get_hate_speech(\n",
    "    sentences, sentiment_df, label_col: str = \"label\"\n",
    "):\n",
    "    \"Get the hate speech of a list of sentences.\"\n",
    "    \n",
    "    hate_model_path = \"Hate-speech-CNERG/dehatebert-mono-german\"\n",
    "    hate_task = pipeline(\n",
    "        \"text-classification\", model=hate_model_path, tokenizer=hate_model_path\n",
    "    )\n",
    "    hate_outputs = [\n",
    "        hate_task(sentence) for sentence in tqdm(sentences, desc=\"Hate speech analysis\")\n",
    "    ]\n",
    "    hate_dict = dict(label=[], score=[], sentence=[])\n",
    "    for idx, output in enumerate(hate_outputs):\n",
    "        hate_dict[\"label\"].append(output[0][\"label\"])\n",
    "        hate_dict[\"score\"].append(output[0][\"score\"])\n",
    "        hate_dict[\"sentence\"].append(sentences[idx])\n",
    "    hate_df = pd.DataFrame(hate_dict)\n",
    "    hate_df[\"label\"] = hate_df.label.map(dict(HATE=\"negativ\", NON_HATE=\"neutral\"))\n",
    "    hate_condition = (hate_df.label == \"negativ\") & (sentiment_df[label_col] == \"negativ\")\n",
    "    hate_df.loc[hate_condition, \"label\"] = \"negativ\"\n",
    "    hate_df.loc[~hate_condition, \"label\"] = \"neutral\"\n",
    "    return hate_df\n",
    "\n",
    "def get_sentiment(sentences):\n",
    "    \"based on pretrained model try to identify which elements have which sentiment\"\n",
    "    sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",use_fast=False)\n",
    "    sentiment_task = pipeline(\n",
    "        \"sentiment-analysis\", model=sentiment_model_path, tokenizer=tokenizer\n",
    "    )\n",
    "    sentiment_outputs = [\n",
    "        sentiment_task(sentence)\n",
    "        for sentence in tqdm(sentences, desc=\"Sentiment analysis\")\n",
    "    ]\n",
    "    sentiments_dict = dict(label=[], score=[], sentence=[])\n",
    "    for idx, output in enumerate(sentiment_outputs):\n",
    "        sentiments_dict[\"label\"].append(output[0][\"label\"])\n",
    "        sentiments_dict[\"score\"].append(output[0][\"score\"])\n",
    "        sentiments_dict[\"sentence\"].append(sentences[idx])\n",
    "    sentiment_df = pd.DataFrame(sentiments_dict)\n",
    "    sentiment_df[\"label\"] = sentiment_df.label.map(\n",
    "        dict(positive=\"positiv\", negative=\"negativ\", neutral=\"neutral\")\n",
    "    )\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment analysis: 100%|██████████| 4880/4880 [04:46<00:00, 17.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3633b7ce7cfb4e418e517e05d834d8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619abb619384428c96d8dc3f52e7b259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16be86943cf4175ac9eeffab11315ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450abaef1ec044149153a4efe94701c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfe230ae5d54ab2b0c99038a1921a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hate speech analysis: 100%|██████████| 4880/4880 [03:56<00:00, 20.66it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sentiment_df1 \u001b[39m=\u001b[39m get_sentiment(sentences)     \n\u001b[1;32m      5\u001b[0m hate_df1 \u001b[39m=\u001b[39m get_hate_speech(sentences,sentiment_df1,label_col\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)     \n\u001b[0;32m----> 6\u001b[0m sentiment_df1\u001b[39m.\u001b[39;49mto_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/\u001b[39;49m\u001b[39m{\u001b[39;49;00melement\u001b[39m}\u001b[39;49;00m\u001b[39m_sentiment.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m§\u001b[39;49m\u001b[39m'\u001b[39;49m)     \n\u001b[1;32m      7\u001b[0m hate_df1\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00melement\u001b[39m}\u001b[39;00m\u001b[39m_hate.csv\u001b[39m\u001b[39m\"\u001b[39m,sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m§\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3549\u001b[0m )\n\u001b[0;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3552\u001b[0m     path_or_buf,\n\u001b[1;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[1;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3568\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[1;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[1;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:694\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[0;32m--> 694\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[1;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    698\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:568\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    566\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m--> 568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data'"
     ]
    }
   ],
   "source": [
    "for element in list(filter(None, data_names)):\n",
    "    program_txt = load_markdown_file(f\"raw/{element}.txt\")\n",
    "    sentences = get_sentences(program_txt)\n",
    "    sentiment_df1 = get_sentiment(sentences)     \n",
    "    hate_df1 = get_hate_speech(sentences,sentiment_df1,label_col= \"label\")     \n",
    "    sentiment_df1.to_csv(f\"data/{element}_sentiment.csv\",sep='§')     \n",
    "    hate_df1.to_csv(f\"data/{element}_hate.csv\",sep='§')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
